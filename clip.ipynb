{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/qiaoruzhang/Mixed-Effect-Model-on-Lambert-Transformation/blob/main/clip.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l4lVdGWjb2nU"
      },
      "source": [
        "# Colab FAQ\n",
        "\n",
        "For some basic overview and features offered in Colab notebooks, check out: [Overview of Colaboratory Features](https://colab.research.google.com/notebooks/basic_features_overview.ipynb)\n",
        "\n",
        "You need to use the colab GPU for this assignment by selecting:\n",
        "\n",
        "> **Runtime**   →   **Change runtime type**   →   **Hardware Accelerator: GPU**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Student Name: Qiaoru Zhang\n",
        "\n",
        "Acknowledgment of AI Tool Use and Originality Declaration:\n",
        "\n",
        "I hereby acknowledge that in completing this assignment, I have utilized artificial intelligence tools, including Google and ChatGPT, as per the professor's permission on syllubus. These tools were employed to assist in the research and development of ideas. I affirm that, while these resources provided valuable insights, the work submitted is my own original creation, reflecting my personal analysis and understanding. The use of AI was conducted ethically, adhering to the academic standards of originality and integrity."
      ],
      "metadata": {
        "id": "JNhh3raeH4n6"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dS2oMufeb64g"
      },
      "source": [
        "# Part 4: Connecting Text and Images with CLIP\n",
        "\n",
        "Acknowledgement: This notebook is based on the code from https://colab.research.google.com/github/openai/clip/blob/master/notebooks/Interacting_with_CLIP.ipynb. Credit to\n",
        "OpenAI."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k243qWV0ckpT"
      },
      "source": [
        "## Section I: Interacting with CLIP [0 pts]\n",
        "\n",
        "This is a self-contained notebook that shows how to download and run CLIP models, calculate the similarity between arbitrary image and text inputs, and perform zero-shot image classifications. The next cells will install the clip package and its dependencies, and check if PyTorch 1.7.1 or later is installed.\n",
        "\n",
        "__Note__: Although these steps are done for you, please take a moment to look through them and make sure you understand their purpose! Understanding how CLIP works will help you solve Section II."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "ra4JaYNhctzV",
        "outputId": "24656f30-aabd-486d-f7d4-f72ce321413c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: ftfy in /usr/local/lib/python3.10/dist-packages (6.3.1)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (2024.9.11)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.66.6)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from ftfy) (0.2.13)\n",
            "Collecting git+https://github.com/openai/CLIP.git\n",
            "  Cloning https://github.com/openai/CLIP.git to /tmp/pip-req-build-ovxdzdzx\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/openai/CLIP.git /tmp/pip-req-build-ovxdzdzx\n",
            "  Resolved https://github.com/openai/CLIP.git to commit dcba3cb2e2827b402d2701e7e1c7d9fed8a20ef1\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: ftfy in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (6.3.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (24.1)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (2024.9.11)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (4.66.6)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (2.5.0+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (0.20.0+cu121)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from ftfy->clip==1.0) (0.2.13)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch->clip==1.0) (1.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision->clip==1.0) (1.26.4)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->clip==1.0) (10.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->clip==1.0) (3.0.2)\n"
          ]
        }
      ],
      "source": [
        "! pip install ftfy regex tqdm\n",
        "! pip install git+https://github.com/openai/CLIP.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "8SHbTOTtcyyM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2ed64398-81f4-4a8a-e51d-f2479d69e220"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Torch version: 2.5.0+cu121\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "print(\"Torch version:\", torch.__version__)\n",
        "torch_version = torch.__version__.split(\".\")\n",
        "assert (int(torch_version[0]) == 1 and int(torch_version[1]) >=7) or int(torch_version[0]) > 1, \"PyTorch 1.7.1 or later is required\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xZ_pLusic71v"
      },
      "source": [
        "### Loading the model\n",
        "\n",
        "`clip.available_models()` will list the names of available CLIP models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "2YcRI-G4c9qw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "caa78f06-1998-4658-eac9-3ff9229f725c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['RN50',\n",
              " 'RN101',\n",
              " 'RN50x4',\n",
              " 'RN50x16',\n",
              " 'RN50x64',\n",
              " 'ViT-B/32',\n",
              " 'ViT-B/16',\n",
              " 'ViT-L/14',\n",
              " 'ViT-L/14@336px']"
            ]
          },
          "metadata": {},
          "execution_count": 57
        }
      ],
      "source": [
        "import clip\n",
        "\n",
        "clip.available_models()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "rvnA8SUTdCLm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ebbfcc94-fb42-41ed-871a-2275936f84f1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model parameters: 151,277,313\n",
            "Input resolution: 224\n",
            "Context length: 77\n",
            "Vocab size: 49408\n"
          ]
        }
      ],
      "source": [
        "model, preprocess = clip.load(\"ViT-B/32\")\n",
        "model.cuda().eval()\n",
        "input_resolution = model.visual.input_resolution\n",
        "context_length = model.context_length\n",
        "vocab_size = model.vocab_size\n",
        "\n",
        "print(\"Model parameters:\", f\"{np.sum([int(np.prod(p.shape)) for p in model.parameters()]):,}\")\n",
        "print(\"Input resolution:\", input_resolution)\n",
        "print(\"Context length:\", context_length)\n",
        "print(\"Vocab size:\", vocab_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1mINCZx6dHyV"
      },
      "source": [
        "### Image Preprocessing\n",
        "\n",
        "We resize the input images and center-crop them to conform with the image resolution that the model expects. Before doing so, we will normalize the pixel intensity using the dataset mean and standard deviation.\n",
        "\n",
        "The second return value from `clip.load()` contains a torchvision `Transform` that performs this preprocessing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "TMhXZDymdLSf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "684e7dca-196c-4849-ce8f-a2fc0662a6d3"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Compose(\n",
              "    Resize(size=224, interpolation=bicubic, max_size=None, antialias=True)\n",
              "    CenterCrop(size=(224, 224))\n",
              "    <function _convert_image_to_rgb at 0x7eea19fdd090>\n",
              "    ToTensor()\n",
              "    Normalize(mean=(0.48145466, 0.4578275, 0.40821073), std=(0.26862954, 0.26130258, 0.27577711))\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 59
        }
      ],
      "source": [
        "preprocess"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QhlQSgaVdTNd"
      },
      "source": [
        "### Text Preprocessing\n",
        "\n",
        "We use a case-insensitive tokenizer, which can be invoked using `clip.tokenize()`. By default, the outputs are padded to become 77 tokens long, which is what the CLIP models expects."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "4Bj_5pmmdYbS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "726b8844-9767-4122-ca1d-6192a3594454"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[49406,  3306,  1002,   256, 49407,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0]], dtype=torch.int32)"
            ]
          },
          "metadata": {},
          "execution_count": 60
        }
      ],
      "source": [
        "clip.tokenize(\"Hello World!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UgkekdIUM0Mk"
      },
      "source": [
        "### Setting up input images and texts\n",
        "\n",
        "We are going to feed 8 example images and their textual descriptions to the model, and compare the similarity between the corresponding features.\n",
        "\n",
        "The tokenizer is case-insensitive, and we can freely give any suitable textual descriptions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "mTzpuNvPM4ZM"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import skimage\n",
        "import IPython.display\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "\n",
        "from collections import OrderedDict\n",
        "import torch\n",
        "\n",
        "%matplotlib inline\n",
        "%config InlineBackend.figure_format = 'retina'\n",
        "\n",
        "# images in skimage to use and their textual descriptions\n",
        "descriptions = {\n",
        "    \"page\": \"a page of text about segmentation\",\n",
        "    \"chelsea\": \"a facial photo of a tabby cat\",\n",
        "    \"astronaut\": \"a portrait of an astronaut with the American flag\",\n",
        "    \"rocket\": \"a rocket standing on a launchpad\",\n",
        "    \"motorcycle_right\": \"a red motorcycle standing in a garage\",\n",
        "    \"camera\": \"a person looking at a camera on a tripod\",\n",
        "    \"horse\": \"a black-and-white silhouette of a horse\",\n",
        "    \"coffee\": \"a cup of coffee on a saucer\"\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "_Mk855_SM7iv",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 228
        },
        "outputId": "256c7722-2102-4e7a-d58e-92e26aca6e93"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/root/.cache/scikit-image/0.24.0/data'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-62-ab5ecdc5eb9e>\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mfilename\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfilename\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfilename\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mskimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_dir\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\".png\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\".jpg\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplitext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdescriptions\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/root/.cache/scikit-image/0.24.0/data'"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1600x500 with 0 Axes>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "original_images = []\n",
        "images = []\n",
        "texts = []\n",
        "plt.figure(figsize=(16, 5))\n",
        "\n",
        "for filename in [filename for filename in os.listdir(skimage.data_dir) if filename.endswith(\".png\") or filename.endswith(\".jpg\")]:\n",
        "    name = os.path.splitext(filename)[0]\n",
        "    if name not in descriptions:\n",
        "        continue\n",
        "\n",
        "    image = Image.open(os.path.join(skimage.data_dir, filename)).convert(\"RGB\")\n",
        "\n",
        "    plt.subplot(2, 4, len(images) + 1)\n",
        "    plt.imshow(image)\n",
        "    plt.title(f\"{filename}\\n{descriptions[name]}\")\n",
        "    plt.xticks([])\n",
        "    plt.yticks([])\n",
        "\n",
        "    original_images.append(image)\n",
        "    images.append(preprocess(image))\n",
        "    texts.append(descriptions[name])\n",
        "\n",
        "plt.tight_layout()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2yGO91x2M-E_"
      },
      "source": [
        "### Building features\n",
        "\n",
        "We normalize the images, tokenize each text input, and run the forward pass of the model to get the image and text features."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "sZceaA-FNBbo",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 280
        },
        "outputId": "215a28d4-7373-4d0f-a5f1-819143bb266b"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "need at least one array to stack",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-63-c298484ffc71>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mimage_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtext_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclip\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"This is \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mdesc\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdesc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtexts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/numpy/core/shape_base.py\u001b[0m in \u001b[0;36mstack\u001b[0;34m(arrays, axis, out, dtype, casting)\u001b[0m\n\u001b[1;32m    443\u001b[0m     \u001b[0marrays\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0masanyarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0marr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    444\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 445\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'need at least one array to stack'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    446\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    447\u001b[0m     \u001b[0mshapes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0marr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: need at least one array to stack"
          ]
        }
      ],
      "source": [
        "image_input = torch.tensor(np.stack(images)).cuda()\n",
        "text_tokens = clip.tokenize([\"This is \" + desc for desc in texts]).cuda()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "zdnSyQcwNDT5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 176
        },
        "outputId": "6b83dba3-edb8-45cf-d0fc-61ef18ec754b"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'image_input' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-64-6bc37aecce8e>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mimage_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mtext_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_tokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'image_input' is not defined"
          ]
        }
      ],
      "source": [
        "with torch.no_grad():\n",
        "    image_features = model.encode_image(image_input).float()\n",
        "    text_features = model.encode_text(text_tokens).float()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PS09uzL7NHpW"
      },
      "source": [
        "### Calculating cosine similarity\n",
        "\n",
        "We normalize the features and calculate the dot product of each pair."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "id": "79G5WDMHNNkC"
      },
      "outputs": [],
      "source": [
        "image_features /= image_features.norm(dim=-1, keepdim=True)\n",
        "text_features /= text_features.norm(dim=-1, keepdim=True)\n",
        "similarity = text_features.cpu().numpy() @ image_features.cpu().numpy().T"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "id": "G2KSRXn-NQKy",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "outputId": "23c45456-8ab1-4210-bbbb-7a308ce6a700"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0.5, 1.0, 'Cosine similarity between text and image features')"
            ]
          },
          "metadata": {},
          "execution_count": 66
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Image size of 102641x2254 pixels is too large. It must be less than 2^16 in each direction.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/IPython/core/formatters.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    339\u001b[0m                 \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    340\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 341\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mprinter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    342\u001b[0m             \u001b[0;31m# Finally look for special method names\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    343\u001b[0m             \u001b[0mmethod\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_real_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_method\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/IPython/core/pylabtools.py\u001b[0m in \u001b[0;36mretina_figure\u001b[0;34m(fig, base64, **kwargs)\u001b[0m\n\u001b[1;32m    166\u001b[0m         \u001b[0mbase64\u001b[0m \u001b[0margument\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m     \"\"\"\n\u001b[0;32m--> 168\u001b[0;31m     \u001b[0mpngdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprint_figure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfmt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"retina\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbase64\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    169\u001b[0m     \u001b[0;31m# Make sure that retina_figure acts just like print_figure and returns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m     \u001b[0;31m# None when the figure is empty.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/IPython/core/pylabtools.py\u001b[0m in \u001b[0;36mprint_figure\u001b[0;34m(fig, fmt, bbox_inches, base64, **kwargs)\u001b[0m\n\u001b[1;32m    149\u001b[0m         \u001b[0mFigureCanvasBase\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 151\u001b[0;31m     \u001b[0mfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcanvas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_figure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbytes_io\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    152\u001b[0m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbytes_io\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetvalue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfmt\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'svg'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/matplotlib/backend_bases.py\u001b[0m in \u001b[0;36mprint_figure\u001b[0;34m(self, filename, dpi, facecolor, edgecolor, orientation, format, bbox_inches, pad_inches, bbox_extra_artists, backend, **kwargs)\u001b[0m\n\u001b[1;32m   2185\u001b[0m                 \u001b[0;31m# force the figure dpi to 72), so we need to set it again here.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2186\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mcbook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_setattr_cm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdpi\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdpi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2187\u001b[0;31m                     result = print_method(\n\u001b[0m\u001b[1;32m   2188\u001b[0m                         \u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2189\u001b[0m                         \u001b[0mfacecolor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfacecolor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/matplotlib/backend_bases.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   2041\u001b[0m                 \"bbox_inches_restore\"}\n\u001b[1;32m   2042\u001b[0m             \u001b[0mskip\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptional_kws\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minspect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmeth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2043\u001b[0;31m             print_method = functools.wraps(meth)(lambda *args, **kwargs: meth(\n\u001b[0m\u001b[1;32m   2044\u001b[0m                 *args, **{k: v for k, v in kwargs.items() if k not in skip}))\n\u001b[1;32m   2045\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Let third-parties do as they see fit.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/matplotlib/backends/backend_agg.py\u001b[0m in \u001b[0;36mprint_png\u001b[0;34m(self, filename_or_obj, metadata, pil_kwargs)\u001b[0m\n\u001b[1;32m    495\u001b[0m             \u001b[0;34m*\u001b[0m\u001b[0mmetadata\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mincluding\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mdefault\u001b[0m \u001b[0;34m'Software'\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    496\u001b[0m         \"\"\"\n\u001b[0;32m--> 497\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_print_pil\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename_or_obj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"png\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpil_kwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    498\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    499\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mprint_to_buffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/matplotlib/backends/backend_agg.py\u001b[0m in \u001b[0;36m_print_pil\u001b[0;34m(self, filename_or_obj, fmt, pil_kwargs, metadata)\u001b[0m\n\u001b[1;32m    443\u001b[0m         *pil_kwargs* and *metadata* are forwarded).\n\u001b[1;32m    444\u001b[0m         \"\"\"\n\u001b[0;32m--> 445\u001b[0;31m         \u001b[0mFigureCanvasAgg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    446\u001b[0m         mpl.image.imsave(\n\u001b[1;32m    447\u001b[0m             \u001b[0mfilename_or_obj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuffer_rgba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfmt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morigin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"upper\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/matplotlib/backends/backend_agg.py\u001b[0m in \u001b[0;36mdraw\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    381\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    382\u001b[0m         \u001b[0;31m# docstring inherited\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 383\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrenderer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_renderer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    384\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrenderer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    385\u001b[0m         \u001b[0;31m# Acquire a lock on the shared font cache.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/matplotlib/backends/backend_agg.py\u001b[0m in \u001b[0;36mget_renderer\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    396\u001b[0m         \u001b[0mreuse_renderer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lastKey\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    397\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mreuse_renderer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 398\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrenderer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRendererAgg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdpi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    399\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lastKey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    400\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrenderer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/matplotlib/backends/backend_agg.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, width, height, dpi)\u001b[0m\n\u001b[1;32m     68\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwidth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwidth\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheight\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mheight\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_renderer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_RendererAgg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwidth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mheight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdpi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filter_renderers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Image size of 102641x2254 pixels is too large. It must be less than 2^16 in each direction."
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 2000x1400 with 1 Axes>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "count = len(descriptions)\n",
        "\n",
        "plt.figure(figsize=(20, 14))\n",
        "plt.imshow(similarity, vmin=0.1, vmax=0.3)\n",
        "plt.yticks(range(count), texts, fontsize=18)\n",
        "plt.xticks([])\n",
        "for i, image in enumerate(original_images):\n",
        "    plt.imshow(image, extent=(i - 0.5, i + 0.5, -1.6, -0.6), origin=\"lower\")\n",
        "for x in range(similarity.shape[1]):\n",
        "    for y in range(similarity.shape[0]):\n",
        "        plt.text(x, y, f\"{similarity[y, x]:.2f}\", ha=\"center\", va=\"center\", size=12)\n",
        "\n",
        "for side in [\"left\", \"top\", \"right\", \"bottom\"]:\n",
        "  plt.gca().spines[side].set_visible(False)\n",
        "\n",
        "plt.xlim([-0.5, count - 0.5])\n",
        "plt.ylim([count + 0.5, -2])\n",
        "\n",
        "plt.title(\"Cosine similarity between text and image features\", size=20)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tOJcirVSNW8v"
      },
      "source": [
        "### Zero-Shot Image Classification\n",
        "\n",
        "You can classify images using the cosine similarity (times 100) as the logits to the softmax operation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "id": "zOeXZ8whNbF6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1ce9f653-8310-4c23-fc40-08833a626e20"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n"
          ]
        }
      ],
      "source": [
        "from torchvision.datasets import CIFAR100\n",
        "\n",
        "cifar100 = CIFAR100(os.path.expanduser(\"~/.cache\"), transform=preprocess, download=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "id": "PtdOIk-8Nik2"
      },
      "outputs": [],
      "source": [
        "text_descriptions = [f\"This is a photo of a  {label}\" for label in cifar100.classes]\n",
        "text_tokens = clip.tokenize(text_descriptions).cuda()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "id": "LcU94r8LNkeQ"
      },
      "outputs": [],
      "source": [
        "with torch.no_grad():\n",
        "    text_features = model.encode_text(text_tokens).float()\n",
        "    text_features /= text_features.norm(dim=-1, keepdim=True)\n",
        "\n",
        "text_probs = (100.0 * image_features @ text_features.T).softmax(dim=-1)\n",
        "top_probs, top_labels = text_probs.cpu().topk(5, dim=-1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "id": "J_Xn9NT1Nnqw",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "10d20885-63fe-4645-c963-6f67d8f6991a"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1600x1600 with 0 Axes>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "plt.figure(figsize=(16, 16))\n",
        "\n",
        "for i, image in enumerate(original_images):\n",
        "    plt.subplot(4, 4, 2 * i + 1)\n",
        "    plt.imshow(image)\n",
        "    plt.axis(\"off\")\n",
        "\n",
        "    plt.subplot(4, 4, 2 * i + 2)\n",
        "    y = np.arange(top_probs.shape[-1])\n",
        "    plt.grid()\n",
        "    plt.barh(y, top_probs[i])\n",
        "    plt.gca().invert_yaxis()\n",
        "    plt.gca().set_axisbelow(True)\n",
        "    plt.yticks(y, [cifar100.classes[index] for index in top_labels[i].numpy()])\n",
        "    plt.xlabel(\"probability\")\n",
        "\n",
        "plt.subplots_adjust(wspace=0.5)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VU9iToj8sCCJ"
      },
      "source": [
        "## Section II: Let's do a Scavenger Hunt! [1 pt]\n",
        "\n",
        "Now, lets see if we can write a caption to retrieve a particular image using CLIP. Specifically, your task is to __write a caption that best matches the following image__:\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?id=1KxvxIO_2HIT7H6WWTwApz7OucmcYah3k\" height=256px>\n",
        "\n",
        "You can think of your caption as a _prompt_ to the model, designed to retrieve the above image from a large collection of images. As you will see, getting this prompt correct can be tricky! To test your caption, we will use it to retrieve images from ImageNet with CLIP. You should continue to tweak your caption until the above image is returned!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_lZqr1BPsbMJ"
      },
      "source": [
        "First, we will download a subset of ImageNet called _\"Tiny ImageNet\"_. Tiny ImageNet has only 200 classes, with each class having 500 traininig images, 50 validation images and 50 test images."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "id": "GhkxW3XSqEr5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0856be13-c129-4ef5-a392-85c0d829e163"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'IMagenet' already exists and is not an empty directory.\n"
          ]
        }
      ],
      "source": [
        "! git clone https://github.com/seshuad/IMagenet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p2_SKdvisoef"
      },
      "source": [
        "In order to reduce time & memory consumption, we will consider only __one__ category of images from the train set (can you guess what this category corresponds to?) Run the following cell to load these images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "id": "PKRixiWmb_Ug"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "img_paths = []\n",
        "for rootdir, subdir, filenames in os.walk(\"/content/IMagenet/tiny-imagenet-200/train/n09256479/images\"):\n",
        "    for file_ in filenames:\n",
        "        if not file_.endswith(\".JPEG\"):\n",
        "            continue\n",
        "        img_paths.append(os.path.join(rootdir, file_))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "__[1pt] Write your caption below.__ Remember, you should think of this caption like a _prompt_; you want to write a caption that will best match the above image under the CLIP model.\n",
        "\n",
        "> __Hint__: Be specific! This works best when your caption is short, but descriptive and specific."
      ],
      "metadata": {
        "id": "NlWfF-urTadS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "id": "p9yw5gGRgmAm"
      },
      "outputs": [],
      "source": [
        "caption = \"A close-up of a group of clownfish swimming together in a coral reef, with orange and white stripes visible.\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OwQlYxfDxLSS"
      },
      "source": [
        "Below, we will display the image that produces the highest probability under the model given your caption. You should continue to tweak your caption until the target image (see above) is produced!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "id": "dTmZio-qiWdc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "outputId": "0f638b28-30de-4d19-fd95-a5ea544265bf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/IMagenet/tiny-imagenet-200/train/n09256479/images/n09256479_452.JPEG\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "image/png": {
              "width": 389,
              "height": 389
            }
          }
        }
      ],
      "source": [
        "original_images = []\n",
        "images = []\n",
        "\n",
        "for img_path in img_paths:\n",
        "\n",
        "    image = Image.open(img_path).convert(\"RGB\")\n",
        "    original_images.append(image)\n",
        "    images.append(preprocess(image))\n",
        "\n",
        "image_input = torch.tensor(np.stack(images)).cuda()\n",
        "with torch.no_grad():\n",
        "    image_features = model.encode_image(image_input).float()\n",
        "image_features /= image_features.norm(dim=-1, keepdim=True)\n",
        "\n",
        "# Delete these to avoid consuming up too much memory\n",
        "del images\n",
        "del image_input\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "text_tokens = clip.tokenize(caption).cuda()\n",
        "\n",
        "with torch.no_grad():\n",
        "    text_features = model.encode_text(text_tokens).float()\n",
        "    text_features /= text_features.norm(dim=-1, keepdim=True)\n",
        "\n",
        "text_probs = (100.0 * image_features @ text_features.T).softmax(dim=0).cpu().detach().numpy()\n",
        "highest_prob = np.argmax(text_probs)\n",
        "plt.axis('off')\n",
        "plt.imshow(original_images[highest_prob])\n",
        "\n",
        "print(img_paths[highest_prob])"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "l4lVdGWjb2nU"
      ],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}